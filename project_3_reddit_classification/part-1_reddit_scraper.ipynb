{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSI Project 3 - Natural Language Processing via Reddit Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A psychology professor is interested in gauging maliciousness via language. Being a frequent surfer of reddit, he has chanced upon two subreddits that would provide him a good metric for comparison: r/pettyrevenge and r/prorevenge. \n",
    "\n",
    "However, this psychology professor has no background in language processing via code, and has thus hired me, a freelance data analyst, to use scraped data from both these subreddits to build a model that would determine if a post was less (r/pettyrevenge) or more (r/prorevenge) malicious, as well as do a simple analysis of any significant words found.\n",
    "\n",
    "The professor has stated that for the purpose of his research, the model needs to achieve an accuracy score of over 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subreddits r/pettyrevenge and r/prorevenge were scraped for about 800 posts each via the PushshiftAPI. The \"selftext\" and \"title\" data categories were then extracted and combined into a \"text\" category. The text was run through a function removing stopwords, punctuation and short words (len < 3), and then lemmatized using the WordNet Lemmatizer. \n",
    "\n",
    "The text was transformed using a Count Vectorizer, and the top scoring words for both reddits were removed before running the data through three classifiers: Multinomial Naive Bayes, K-Nearest-Neighbors and Random Forest, each with minor optimisation via GridSearchCV. Random Forest scored the best in accuracy overall, and was then further optimised, resulting in a model with 72.7% accuracy. \n",
    "\n",
    "After examination of the highest weighted features, it was theorized that there were several themes that significantly factored into causing someone to increase their maliciousness: Level of hostile intent from the offending party, level of desperation of the victim and forward planning for the revenge.\n",
    "\n",
    "Further research was suggested using several other \"revenge\" subreddits, such as r/nuclearrevenge, so that one's maliciousness can be determined on a scale rather than as a binary function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Scraping Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "Part 1 - Reddit Scraping\n",
    "\n",
    "- [Subreddit Overviews](#Subreddit-Overviews)\n",
    "    - [r/pettyrevenge](#r/pettyrevenge)\n",
    "    - [r/prorevenge](#r/prorevenge)\n",
    "\n",
    "\n",
    "- [The Reddit API](#The-Reddit-API)\n",
    "\n",
    "\n",
    "- [Alternative to the Reddit API: The Pushshift API](#Alternative-to-the-Reddit-API:-The-Pushshift-API)\n",
    "\n",
    "\n",
    "- [Scraping Reddit using the PushshiftAPI](#Scrapping-Reddit-using-the-PushshiftAPI)\n",
    "\t- [Scraping Data](#Scraping-Data)\n",
    "    \n",
    "<a href = \"part-2_eda_and_data_cleaning.ipynb\">Part 2 - EDA and Data Cleaning</a><br>\n",
    "\n",
    "<a href = \"part-3_model_building.ipynb\">Part 3 - Model Building</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit Overviews\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide some context to the problem statement, a quick overview of both subreddits have been appended below. Both subreddits share several characteristics, only seeming to  differ in the amount of malice demonstracted by the submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r/pettyrevenge\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/petty.png\" width =\"600\" height = \"400\" style=\"border:1px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.reddit.com/r/pettyrevenge\">/r/pettyrevenge</a> is subreddit where submissions consist of stories of how people have done really petty things to get back at other people. The sub currently has 975k members, and has been in existence since 2012.  A common characteristic of the posts are the tl;dr (too long, didn't read section, which consists of a summary of the entire posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r/prorevenge \n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pro.png\" width =\"600\" height = \"400\" style=\"border:1px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.reddit.com/r/ProRevenge\">/r/prorevenge</a> is subreddit where submissions consist of stories of how people have gone through extreme lengths to get back at other people. The sub currently has 1.1 million members. Similar to r/pettyrevenge, this subreddit has been existence since 2012, and also share the characteristic of posts with tl;dr sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reddit API\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/redditapi2.png\" width =\"600\" height = \"400\" style=\"border:1px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit provides an easy to access API via the addition of \".json\" to the end of any of their subreddits. This generates a json-formatted file, which is a language-independent data exchange format. In a python specific context, this mean a list of dictionaries,  which one would then be able to navigate through to extract the required information. \n",
    "\n",
    "The major disadvange of the reddit API is readability. As seen in the image above, the reddit API's json output is a large chunk of text, without any line spacing and/or formatting. While it would be possible to manually scan every line of code to find the exact combination of keys required to extract infomration, this would be time consuming and labor intensive.\n",
    "\n",
    "There are better alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to the Reddit API: The Pushshift API\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pushshiftapi.png\" width =\"600\" height = \"400\" style=\"border:1px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several alternatives to the reddit API exist. One such popular API is the <a href=\"https://pushshift.io/api-parameters/\">PushshiftAPI</a>. The PushshiftAPI converts the Reddit json output into a more readable format (as shown in the image above), allowing for an easier time in scanning through the document. \n",
    "\n",
    "There is one disadvantage to using the PushshiftAPI. Unlike the Reddit API, the PushshiftAPI is not \"real-time\", as it functions by copying and re-formatting the RedditAPI. This means that recent posts and edits up to a month before \"now\" might not be reflected in the PushshiftAPI json file.\n",
    "\n",
    "For the current use case, however, there is no significant effect on model building, and the PushshiftAPI will be utilized for our data extraction process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit using the PushshiftAPI\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below makes use of the PushshiftAPI to scrape posts by date, utilizing the \"created_utc\" information of the last post to determine the start date of the next set of posts to scrape. Arbitarily, a minimum of 800 posts were scraped from each subreddit. \n",
    "\n",
    "An initial stage of data cleaning was also integrated into the scrapping process by examining each post to check if it was either \"removed\" or containd no text. This resulted in a dataset of 880 posts from r/prorevenge and 813 posts from r/pettyrevenge, which was then saved to a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Data\n",
    "[top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the start and stop dates and converting them into timestamps\n",
    "\n",
    "start_date = datetime(2019, 1,1)\n",
    "stop_date = datetime(2020, 1,1)\n",
    "\n",
    "start_timestamp = int(datetime.timestamp(start_date))\n",
    "stop_timestamp = int(datetime.timestamp(stop_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a function to scrape reddit for a mminimum number of posts\n",
    "\n",
    "def pushscraper(start_timestamp, stop_timestamp, subreddit, count_limit):\n",
    "    \n",
    "    # Creating an empty aggregated posts list to append posts to later \n",
    "    \n",
    "    aggregate = []\n",
    "    \n",
    "    subCount = 0\n",
    "    \n",
    "    # Internal function to read and extract the data from the json input\n",
    "    \n",
    "    def getPSData(start_timestamp, stop_timestamp, subreddit):\n",
    "        \n",
    "        # The url is based on the multiple endpoints of the PushshiftAPI\n",
    "        \n",
    "        url = 'https://api.pushshift.io/reddit/search/submission/?size=1000&after='+str(start_timestamp)+'&before='+str(stop_timestamp)+'&subreddit='+str(subreddit)\n",
    "        print(url)\n",
    "        \n",
    "        # Using the requests and json modules to convert the json input into a pandas dataframe\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        data = json.loads(r.text)\n",
    "        return data['data']    \n",
    "    \n",
    "    # Calling getPSData to get the initial set of posts\n",
    "    \n",
    "    data = getPSData(start_timestamp, stop_timestamp, subreddit)\n",
    "    \n",
    "    # While loop to run the function continouusly until the required number of posts is met (count_limit)\n",
    "\n",
    "    while len(data) > 0 and subCount < count_limit:\n",
    "        \n",
    "        # A Try/Except and If/Else is required to exclude posts that were either removed or contain no text\n",
    "        \n",
    "        for submission in data:\n",
    "            try:\n",
    "                if submission[\"selftext\"] != \"[removed]\":\n",
    "                    subCount+=1\n",
    "                    aggregate.append(submission)\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Printing number of posts scraped and datetime of last post\n",
    "        \n",
    "        print(f'total posts scraped: {subCount}')\n",
    "        print(str(datetime.fromtimestamp(data[-1]['created_utc'])))               \n",
    "        \n",
    "        # Calling getPSData with the created date of the last submission\n",
    "        \n",
    "        start_timestamp = data[-1]['created_utc']        \n",
    "        data = getPSData(start_timestamp, stop_timestamp, subreddit)\n",
    "    \n",
    "    result = pd.DataFrame(aggregate)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546272000&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 95\n",
      "2019-01-15 14:06:51\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547532411&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 193\n",
      "2019-01-30 04:54:12\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548795252&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 292\n",
      "2019-02-13 23:00:59\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550070059&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 389\n",
      "2019-02-17 16:54:21\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550393661&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 487\n",
      "2019-02-19 03:02:46\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550516566&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 586\n",
      "2019-02-20 06:53:42\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550616822&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 686\n",
      "2019-02-21 09:59:03\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550714343&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 782\n",
      "2019-02-22 07:36:16\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550792176&before=1577808000&subreddit=prorevenge\n",
      "total posts scraped: 880\n",
      "2019-02-23 12:46:02\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550897162&before=1577808000&subreddit=prorevenge\n"
     ]
    }
   ],
   "source": [
    "# Scraping r/prorevenge for 800 posts\n",
    "\n",
    "pro_df = pushscraper(start_timestamp, stop_timestamp, \"prorevenge\", 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546272000&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 29\n",
      "2019-01-05 11:01:43\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546657303&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 58\n",
      "2019-01-11 04:09:10\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547150950&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 89\n",
      "2019-01-15 20:36:15\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547555775&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 121\n",
      "2019-01-19 05:48:21\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1547848101&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 145\n",
      "2019-01-22 12:26:25\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548131185&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 179\n",
      "2019-01-25 04:10:17\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548360617&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 205\n",
      "2019-01-28 08:17:11\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548634631&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 239\n",
      "2019-01-31 10:40:37\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1548902437&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 268\n",
      "2019-02-04 02:03:30\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549217010&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 290\n",
      "2019-02-06 12:06:16\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549425976&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 319\n",
      "2019-02-09 10:06:57\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549678017&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 345\n",
      "2019-02-12 03:24:17\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1549913057&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 359\n",
      "2019-02-13 15:55:06\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550044506&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 383\n",
      "2019-02-15 11:11:44\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550200304&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 408\n",
      "2019-02-17 09:33:45\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550367225&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 434\n",
      "2019-02-18 22:46:19\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550501179&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 458\n",
      "2019-02-19 22:42:26\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550587346&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 491\n",
      "2019-02-21 01:38:47\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550684327&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 510\n",
      "2019-02-21 20:51:53\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550753513&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 537\n",
      "2019-02-22 21:26:10\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550841970&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 562\n",
      "2019-02-23 23:32:01\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1550935921&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 579\n",
      "2019-02-24 20:12:20\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551010340&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 603\n",
      "2019-02-25 15:31:56\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551079916&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 622\n",
      "2019-02-26 19:05:57\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551179157&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 644\n",
      "2019-02-27 15:28:57\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551252537&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 662\n",
      "2019-02-28 11:42:32\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551325352&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 679\n",
      "2019-03-01 18:44:26\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551437066&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 702\n",
      "2019-03-02 20:27:49\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551529669&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 723\n",
      "2019-03-03 17:18:55\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551604735&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 748\n",
      "2019-03-04 19:55:00\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551700500&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 766\n",
      "2019-03-05 18:47:49\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551782869&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 783\n",
      "2019-03-06 13:18:57\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551849537&before=1577808000&subreddit=pettyrevenge\n",
      "total posts scraped: 813\n",
      "2019-03-07 18:55:42\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1551956142&before=1577808000&subreddit=pettyrevenge\n"
     ]
    }
   ],
   "source": [
    "# Scraping r/pettyrevenge for 800 posts\n",
    "\n",
    "petty_df = pushscraper(start_timestamp, stop_timestamp, \"pettyrevenge\", 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining posts and saving as a csv file\n",
    "\n",
    "final_posts_ps = pd.concat([petty_df, pro_df])\n",
    "\n",
    "pd.DataFrame(final_posts_ps).to_csv(f'data/raw_scrape_ps.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crosscheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/raw_scrape_ps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1693, 56)\n"
     ]
    }
   ],
   "source": [
    "test.drop_duplicates(inplace = True)\n",
    "print (test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>...</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>updated_utc</th>\n",
       "      <th>gilded</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "      <th>edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr_kitloin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_wupes</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1546273636</td>\n",
       "      <td>...</td>\n",
       "      <td>No mike and ikes for me? How about hot tamales...</td>\n",
       "      <td>https://www.reddit.com/r/pettyrevenge/comments...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CyberneticFennec</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_15fz43</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1546280455</td>\n",
       "      <td>...</td>\n",
       "      <td>I'm not allowed to use the car? Fine.</td>\n",
       "      <td>https://www.reddit.com/r/pettyrevenge/comments...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bblackwalker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_24uomksc</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1546291051</td>\n",
       "      <td>...</td>\n",
       "      <td>Tinder date was an extreme feminist and wanted...</td>\n",
       "      <td>https://www.reddit.com/r/pettyrevenge/comments...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ilovemysubaru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_11xnqyar</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1546305143</td>\n",
       "      <td>...</td>\n",
       "      <td>Pretty Revenge Request</td>\n",
       "      <td>https://www.reddit.com/r/pettyrevenge/comments...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shinn_ann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_p5svqtv</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1546309401</td>\n",
       "      <td>...</td>\n",
       "      <td>refuse to quiet down? let your kids learn rap ...</td>\n",
       "      <td>https://www.reddit.com/r/pettyrevenge/comments...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author  author_flair_css_class author_flair_richtext  \\\n",
       "0        Mr_kitloin                     NaN                    []   \n",
       "1  CyberneticFennec                     NaN                    []   \n",
       "2      bblackwalker                     NaN                    []   \n",
       "3     ilovemysubaru                     NaN                    []   \n",
       "4         shinn_ann                     NaN                    []   \n",
       "\n",
       "   author_flair_text author_flair_type author_fullname  author_patreon_flair  \\\n",
       "0                NaN              text        t2_wupes                 False   \n",
       "1                NaN              text       t2_15fz43                 False   \n",
       "2                NaN              text     t2_24uomksc                 False   \n",
       "3                NaN              text     t2_11xnqyar                 False   \n",
       "4                NaN              text      t2_p5svqtv                 False   \n",
       "\n",
       "   can_mod_post  contest_mode  created_utc  ...  \\\n",
       "0         False         False   1546273636  ...   \n",
       "1         False         False   1546280455  ...   \n",
       "2         False         False   1546291051  ...   \n",
       "3         False         False   1546305143  ...   \n",
       "4         False         False   1546309401  ...   \n",
       "\n",
       "                                               title  \\\n",
       "0  No mike and ikes for me? How about hot tamales...   \n",
       "1              I'm not allowed to use the car? Fine.   \n",
       "2  Tinder date was an extreme feminist and wanted...   \n",
       "3                             Pretty Revenge Request   \n",
       "4  refuse to quiet down? let your kids learn rap ...   \n",
       "\n",
       "                                                 url whitelist_status wls  \\\n",
       "0  https://www.reddit.com/r/pettyrevenge/comments...          all_ads   6   \n",
       "1  https://www.reddit.com/r/pettyrevenge/comments...          all_ads   6   \n",
       "2  https://www.reddit.com/r/pettyrevenge/comments...          all_ads   6   \n",
       "3  https://www.reddit.com/r/pettyrevenge/comments...          all_ads   6   \n",
       "4  https://www.reddit.com/r/pettyrevenge/comments...          all_ads   6   \n",
       "\n",
       "   author_cakeday  updated_utc  gilded  post_hint  preview  edited  \n",
       "0             NaN          NaN     NaN        NaN      NaN     NaN  \n",
       "1             NaN          NaN     NaN        NaN      NaN     NaN  \n",
       "2             NaN          NaN     NaN        NaN      NaN     NaN  \n",
       "3             NaN          NaN     NaN        NaN      NaN     NaN  \n",
       "4             NaN          NaN     NaN        NaN      NaN     NaN  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
