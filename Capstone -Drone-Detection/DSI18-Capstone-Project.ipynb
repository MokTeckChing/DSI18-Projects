{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSI18 Capstone Project:\n",
    "## Drone detection in images and videos using YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits to <a href = \"https://github.com/ultralytics/yolov5\">ultralytics</a> and <a href =\"https://roboflow.com/\">Roboflow</a> for the YOLOv5 and Google Colab code, and to <a href = \"https://github.com/chuanenlin/drone-net\"> chuan en lin </a> for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "There is an increasing availability of small, cheap unmanned aerial vehicles equipped with cameras. As such, there is a corresponding requirement for a low-cost, quick and easily deployable system to alert security stakeholders of the presence of such unmanned vehicles so that further actions can be taken.\n",
    "\n",
    "# Executive Summary\n",
    "A dataset of 2492 drone images were uploaded to Roboflow. The dataset was then examined and any incorrect bounding boxes corrected and inappropriate images removed. The images were then further transformed, resulting in a final dataset of 4984 images. A custom YOLOv5m model was then trained on the dataset using Google Colab, resulting in a high score of 0.985 mAP. The best weights were then saved, downloaded and utilized to create a standalone deployment model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - EDA / Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drone image dataset was obtained from Chuan En Lin (credited at the start of the notebook). The dataset consisted of 2664 images along with associated labels that were normalized to image size, as per YOLO image labeling requiremnets. \n",
    "\n",
    "However, a quick examination of the dataset revealed several issues that require further rectification: \n",
    "- Several of the label text files had multiple bounding boxes, which did not correlate to the location of the drones.\n",
    "- Several of the images had incorrect bounding boxes. \n",
    "- Several images were not representative of the problem statements. \n",
    "\n",
    "The images and labels were uploaded to <a href=\"https://roboflow.ai\">roboflow</a>, an online image store/labeller for image detection datasets.\n",
    "\n",
    "The dataset was then manually examined, and images were removed/bounding boxes were rectified as required. This resulted in 2492 remaining images. Roboflow also automated the creation of transformed images, as well as train/validation/test splits. \n",
    "\n",
    "This reuslted in a final dataset of 4984 images (3508 train, 994 validation, 498 test).\n",
    "\n",
    "Unfortunately due to size constraints for free accounts, the final dataset had to be removed from roboflow, but will be available in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was written to strip the label txt files of additional bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the file paths\n",
    "mypath = \"raw_data/drones/normalized-labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of files and removing the \".DS_Store\" file (a Mac only issue)\n",
    "filelist = [i for i in listdir(mypath)]\n",
    "filelist.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripping lines after the first line\n",
    "for i in filelist:\n",
    "    f = open(mypath + i, \"r+\")\n",
    "    text = f.read()\n",
    "    result = (\"drone\" + text[1:])\n",
    "    f.seek(0)\n",
    "    f.write(result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Incorrect bounding boxes\n",
    "\n",
    "The images below are examples of images that were incorrectly bounded. These bounding boxes were manually re-drawn in roboflow.\n",
    "<br>\n",
    "<br>\n",
    "<img src = \"images/bb1.png\">\n",
    "<img src = \"images/bb2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Misrepresentative images\n",
    "\n",
    "The images below are examples of images that were misrepresentative of the data needed. Images in this category include extreme closeups of drones or bounding boxes that include items other than drones. These images were selective re-bounded or removed. \n",
    "<br>\n",
    "<br>\n",
    "<img src = \"images/wc1.png\">\n",
    "<img src = \"images/wc2.png\">\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Model Training and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the model was done using google colab.\n",
    "\n",
    "Please refer to the colab workbook link <a href = \"https://colab.research.google.com/drive/1KI_WCsNCKvP-nLvOPKc6DnrUh6_0aEPt?usp=sharing\">here</a>. \n",
    "\n",
    "Do note that the direct link for the image dataset from roboflow was removed due to security concerns, so the notebook cannot be run as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cloned YOLOv5 repo, while not large, consistes of a number of extraneous files that were removed (e.g. train.py, test.py). A custom GUI, as well as new input and output locations, were then added to the production model's detect.py file. The default weights and values were also amended accordingly. The requirements.txt file was also amended.\n",
    "\n",
    "The final size of the application is 42.8 mb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example detect.py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from gooey import Gooey\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, non_max_suppression, scale_coords, \\\n",
    "    xyxy2xywh, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, time_synchronized\n",
    "\n",
    "\n",
    "def detect(opt, save_img=False):\n",
    "    source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://'))\n",
    "\n",
    "    # Directories\n",
    "    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device(opt.device)\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        view_img = True\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz)\n",
    "    else:\n",
    "        save_img = True\n",
    "        dataset = LoadImages(source, img_size=imgsz)\n",
    "\n",
    "    # Get names and colors\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "    # Run inference\n",
    "    t0 = time.time()\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "            else:\n",
    "                p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # img.jpg\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += f'{n} {names[int(c)]}s, '  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
    "                        with open(txt_path + '.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    if save_img or view_img:  # Add bbox to image\n",
    "                        label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "            # Print time (inference + NMS)\n",
    "            print(f'{s}Done. ({t2 - t1:.3f}s)')\n",
    "\n",
    "            # Stream results\n",
    "            if view_img:\n",
    "                cv2.imshow(str(p), im0)\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                else:  # 'video'\n",
    "                    if vid_path != save_path:  # new video\n",
    "                        vid_path = save_path\n",
    "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                            vid_writer.release()  # release previous video writer\n",
    "\n",
    "                        fourcc = 'mp4v'  # output video codec\n",
    "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n",
    "                    vid_writer.write(im0)\n",
    "\n",
    "    if save_txt or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        print(f\"Results saved to {save_dir}{s}\")\n",
    "\n",
    "    print(f'Done. ({time.time() - t0:.3f}s)')\n",
    "\n",
    "@Gooey()\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default='weights.pt', help='model.pt path(s)')\n",
    "    parser.add_argument('--source', type=str, default='../input', help='source')  # file/folder, 0 for webcam\n",
    "    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.70, help='object confidence threshold')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--view-img', action='store_true', help='display results')\n",
    "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
    "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "    parser.add_argument('--project', default='../output', help='save results to project/name')\n",
    "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    opt = parser.parse_args()\n",
    "    print(opt)\n",
    "    check_requirements()\n",
    "\n",
    "    detect(opt = opt)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Conclusion / Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassification examination\n",
    "\n",
    "While the model was successful in classifying and localizing drones, several issues became apparent when examining the misclassifications. \n",
    "\n",
    "Firstly, the model seemed to have trouble with blurry images.\n",
    "\n",
    "<img src = \"images/pred1.jpg\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Secondly, the model has issues with non-DJI drones.\n",
    "\n",
    "<img src = \"images/predlow.jpg\">\n",
    "<br>\n",
    "<br>\n",
    "A further examination of the dataset leads to the conclusion that these misclassifications were casued by insufficient data in the dataset. While the dataset consisted of a large number of images, they mostly consisted of DJI drones (mostly featuing 4 rotors and an underslung camera) that were shot at similar angles and ranges. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Research/ Production Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the production model works as a standalone system, more work will need to be done to tie in the detections with a customer's alerting system/workflow. \n",
    "\n",
    "Additionally, the availability of several versions of YOLOv5 means that any such model might have to be retrained on bigger/smaller networks to suit the customer's hardware requriements and operational needs. Further tweaking of both the IOU and confidence thresholds might also be a requirement. \n",
    "\n",
    "Finally, if possible, a larger dataset should be gathered of drones at different ranges in different orientations, as well as non-DJI drones. This will increase the generalizability of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
